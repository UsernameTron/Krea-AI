#!/usr/bin/env python3
"""
Test Memory Fix for FLUX.1 Krea Pipeline Loading
Quick validation of memory settings
"""

import torch
import os
from diffusers import FluxPipeline

# Set memory-safe environment
os.environ['PYTORCH_MPS_MEMORY_FRACTION'] = '0.8'
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # Disable limit
os.environ['PYTORCH_MPS_LOW_WATERMARK_RATIO'] = '0.6'

def test_pipeline_loading():
    """Test if pipeline can load without memory errors"""
    print("üß™ Testing FLUX.1 Krea Pipeline Loading with Memory Fixes")
    print("=" * 60)
    
    print(f"üì± PyTorch: {torch.__version__}")
    print(f"üéØ MPS Available: {'‚úÖ' if torch.backends.mps.is_available() else '‚ùå'}")
    print(f"üíæ Memory Fraction: {os.environ.get('PYTORCH_MPS_MEMORY_FRACTION')}")
    print(f"üîß High Watermark: {os.environ.get('PYTORCH_MPS_HIGH_WATERMARK_RATIO')}")
    
    try:
        print(f"\nüì• Loading pipeline...")
        
        # Clear cache before loading
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        # Load pipeline with memory fixes
        pipeline = FluxPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-Krea-dev",
            torch_dtype=torch.float16,
            use_safetensors=True,
            low_cpu_mem_usage=True
        )
        
        print("‚úÖ Pipeline loaded successfully!")
        
        # Move to MPS
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        if device == "mps":
            torch.mps.empty_cache()
        
        pipeline = pipeline.to(device)
        print(f"‚úÖ Pipeline moved to {device}")
        
        # Test optimizations
        try:
            pipeline.enable_model_cpu_offload()
            print("‚úÖ CPU offload enabled")
        except:
            print("‚ö†Ô∏è  CPU offload not available")
        
        try:
            pipeline.enable_attention_slicing("auto")
            print("‚úÖ Attention slicing enabled")
        except:
            print("‚ö†Ô∏è  Attention slicing not available")
        
        try:
            pipeline.enable_vae_slicing()
            print("‚úÖ VAE slicing enabled")
        except:
            print("‚ö†Ô∏è  VAE slicing not available")
        
        try:
            pipeline.enable_vae_tiling()
            print("‚úÖ VAE tiling enabled")  
        except:
            print("‚ö†Ô∏è  VAE tiling not available")
        
        print(f"\nüéâ SUCCESS: Pipeline loaded and optimized!")
        print(f"üí° Memory fixes resolved the 25GB MPS limit issue")
        
        # Cleanup
        del pipeline
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        return True
        
    except Exception as e:
        error_msg = str(e)
        print(f"\n‚ùå FAILED: {error_msg}")
        
        if "out of memory" in error_msg.lower():
            print(f"\nüíæ Memory issue still present:")
            print(f"1. Try: export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0")
            print(f"2. Try: export PYTORCH_MPS_MEMORY_FRACTION=0.7") 
            print(f"3. Close other applications")
        
        return False

if __name__ == "__main__":
    success = test_pipeline_loading()
    if success:
        print(f"\nüöÄ Ready for high-performance generation!")
    else:
        print(f"\nüîß Additional memory tuning needed")