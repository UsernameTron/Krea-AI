{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b9ecd",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport torch\nfrom src.flux.util import load_ae, load_clip, load_t5, load_flow_model\nfrom src.flux.pipeline import Sampler\n\n# M4 Pro Optimizations\nos.environ[\"PYTORCH_MPS_MEMORY_FRACTION\"] = \"0.95\"\nos.environ[\"PYTORCH_MPS_ALLOCATOR_POLICY\"] = \"expandable_segments\"\nos.environ[\"PYTORCH_MPS_PREFER_FAST_ALLOC\"] = \"1\"\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nos.environ[\"OMP_NUM_THREADS\"] = \"12\"  # 8P + 4E cores\nos.environ[\"MKL_NUM_THREADS\"] = \"12\"\n\n# Use MPS (Metal Performance Shaders) for Apple Silicon\nif torch.backends.mps.is_available():\n    device = \"mps\"\n    print(\"‚úÖ Using Metal Performance Shaders (MPS) for Apple Silicon\")\nelif torch.cuda.is_available():\n    device = \"cuda\"\n    print(\"‚úÖ Using CUDA\")\nelse:\n    device = \"cpu\"\n    print(\"‚ö†Ô∏è  Using CPU\")\n\nprint(f\"üöÄ Loading FLUX.1 Krea models on {device}...\")\n\n# Load models to CPU first for memory efficiency\nmodel = load_flow_model(\"flux-krea-dev\", device=\"cpu\")\nae = load_ae(\"flux-krea-dev\")\nclip = load_clip()\nt5 = load_t5()\n\n# Move to device with bfloat16 precision\nprint(\"üì¶ Moving models to device...\")\nae = ae.to(device=device, dtype=torch.bfloat16)\nclip = clip.to(device=device, dtype=torch.bfloat16)\nt5 = t5.to(device=device, dtype=torch.bfloat16)\nmodel = model.to(device, dtype=torch.bfloat16)\n\nprint(\"‚úÖ Models loaded and optimized for M4 Pro!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(\n",
    "    model=model,\n",
    "    ae=ae,\n",
    "    clip=clip,\n",
    "    t5=t5,\n",
    "    device=device,\n",
    "    dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508f5d3",
   "metadata": {},
   "outputs": [],
   "source": "# Clear MPS cache for optimal performance\nif device == \"mps\":\n    torch.mps.empty_cache()\n    torch.mps.synchronize()\n\n# Generate image with M4 Pro optimizations\nimage = sampler(\n    prompt=\"a cute cat\",\n    width=1024,\n    height=1024,\n    guidance=4.5,\n    num_steps=28,\n    seed=42,\n)\n\n# Show performance info\nprint(f\"üöÄ Generation complete on {device}\")\nprint(f\"üíæ Memory usage: {torch.cuda.memory_allocated() / 1024**3:.1f} GB\" if device == \"cuda\" else \"üíæ Using unified memory\")\n\n# Display the cute cat\nimage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d17741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}